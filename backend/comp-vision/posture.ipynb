{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e6380b-dbea-4e94-9a91-032e6601dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 19:17:06.294347: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-06 19:17:07.159411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "# import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "945212ac-8f6a-4d81-9c39-90f14e5fea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a29481-114f-413f-9ec1-dd583218016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "# with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "    \n",
    "#         image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         # image.flags.writable = False\n",
    "\n",
    "#         results = pose.process(image)\n",
    "\n",
    "#         # image.flags.writable = True\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#         mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "#         cv2.imshow('Mediapipe feed', image)\n",
    "        \n",
    "#         if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dac1574-6c0e-43ae-911f-bb0987cc3fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({(15, 21), (16, 20), (18, 20), (3, 7), (14, 16), (23, 25), (28, 30), (11, 23), (27, 31), (6, 8), (15, 17), (24, 26), (16, 22), (4, 5), (5, 6), (29, 31), (12, 24), (23, 24), (0, 1), (9, 10), (1, 2), (0, 4), (11, 13), (30, 32), (28, 32), (15, 19), (16, 18), (25, 27), (26, 28), (12, 14), (17, 19), (2, 3), (11, 12), (27, 29), (13, 15)})\n"
     ]
    }
   ],
   "source": [
    "print(mp_pose.POSE_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4739a450-0604-4863-9309-1f68fe383d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({(15, 21), (16, 20), (18, 20), (3, 7), (14, 16), (23, 25), (28, 30), (11, 23), (27, 31), (6, 8), (15, 17), (24, 26), (16, 22), (4, 5), (5, 6), (29, 31), (12, 24), (23, 24), (0, 1), (9, 10), (1, 2), (0, 4), (11, 13), (30, 32), (28, 32), (15, 19), (16, 18), (25, 27), (26, 28), (12, 14), (17, 19), (2, 3), (11, 12), (27, 29), (13, 15)})\n"
     ]
    }
   ],
   "source": [
    "print(mp.solutions.pose.POSE_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d3a6bc-7d36-44c8-adc8-075081a97ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_in_frame(landmarks):\n",
    "    left = landmarks[mp_pose.PoseLandmark.LEFT_HEEL.value].y\n",
    "    right = landmarks[mp_pose.PoseLandmark.RIGHT_HEEL.value].y\n",
    "    nose = landmarks[mp_pose.PoseLandmark.NOSE.value].y\n",
    "\n",
    "    # heel out of frame\n",
    "    if left > 0.95 or right > 0.95:\n",
    "        return False\n",
    "    # heel wayy to in frame\n",
    "    if left < 0 or right < 0 or nose < 0:\n",
    "        return False\n",
    "    if nose > 0.15:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3930ebf0-6d14-4892-9f8c-ba265fffb18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712456229.331472   71823 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1712456229.357770   71896 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 545.23.08), renderer: NVIDIA GeForce RTX 2060/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "captured_body = False\n",
    "quit = False\n",
    "measurements = []\n",
    "start = None\n",
    "image = None\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # variables to maintain continous stream of data\n",
    "            if not captured_body:\n",
    "                captured_body = True\n",
    "                start = time.time() \n",
    "\n",
    "            # if our body isn't in frame, restart\n",
    "            if not body_in_frame(landmarks):\n",
    "                captured_body = False\n",
    "                start = None\n",
    "                measurements.clear()\n",
    "                \n",
    "            # add landmarks to our current stuff\n",
    "            if captured_body:\n",
    "                measurements.append(landmarks)\n",
    "                \n",
    "                # quit when we have 3 seconds of data\n",
    "                if time.time() - start > 3:\n",
    "                    quit = True\n",
    "            \n",
    "        except:\n",
    "            captured_body = False\n",
    "            start = None\n",
    "            measurements.clear()\n",
    "\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        cv2.imshow('Mediapipe feed', image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q') or quit:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159e1688-ee71-4d64-8b34-36492a8ba1c7",
   "metadata": {},
   "source": [
    "## Turn measurements into a 3d tensor. \n",
    "The tensor should be of shape(`len(measurements)`, `33` (the number of landmarks), `4` (x, y, z, and visisibility))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d38bd27-48a6-4d2d-9331-fe9120b24cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 33, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor_data = torch.zeros(len(measurements), 33, 4)\n",
    "print(tensor_data.shape)\n",
    "\n",
    "for idx, itm in enumerate(measurements):\n",
    "    for jdx, jtm in enumerate(itm):\n",
    "        tensor_data[idx][jdx][0] = jtm.x\n",
    "        tensor_data[idx][jdx][1] = jtm.y\n",
    "        tensor_data[idx][jdx][2] = jtm.z\n",
    "        tensor_data[idx][jdx][3] = jtm.visibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d743e-fe42-46eb-b3d5-aef3290bee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('BGR Image', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bb099fd-b88d-4877-8a76-e7f3cb58e2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4823,  0.0581, -0.4958,  0.9989]) 0\n",
      "tensor([ 0.4909,  0.0398, -0.4750,  0.9988]) 1\n",
      "tensor([ 0.4962,  0.0405, -0.4750,  0.9989]) 2\n",
      "tensor([ 0.5013,  0.0413, -0.4751,  0.9989]) 3\n",
      "tensor([ 0.4755,  0.0383, -0.4754,  0.9986]) 4\n",
      "tensor([ 0.4701,  0.0382, -0.4754,  0.9986]) 5\n",
      "tensor([ 0.4650,  0.0382, -0.4754,  0.9986]) 6\n",
      "tensor([ 0.5093,  0.0497, -0.3149,  0.9990]) 7\n",
      "tensor([ 0.4574,  0.0477, -0.3189,  0.9990]) 8\n",
      "tensor([ 0.4915,  0.0776, -0.4337,  0.9990]) 9\n",
      "tensor([ 0.4713,  0.0772, -0.4347,  0.9988]) 10\n",
      "tensor([ 0.5514,  0.1670, -0.1916,  0.9997]) 11\n",
      "tensor([ 0.4085,  0.1650, -0.1875,  0.9996]) 12\n",
      "tensor([ 0.5962,  0.2997, -0.1281,  0.9886]) 13\n",
      "tensor([ 0.3577,  0.3028, -0.1252,  0.9767]) 14\n",
      "tensor([ 0.6489,  0.4011, -0.2903,  0.9870]) 15\n",
      "tensor([ 0.2990,  0.4072, -0.2688,  0.9760]) 16\n",
      "tensor([ 0.6686,  0.4349, -0.3291,  0.9735]) 17\n",
      "tensor([ 0.2738,  0.4408, -0.3069,  0.9617]) 18\n",
      "tensor([ 0.6660,  0.4340, -0.3914,  0.9759]) 19\n",
      "tensor([ 0.2751,  0.4387, -0.3731,  0.9649]) 20\n",
      "tensor([ 0.6582,  0.4227, -0.3194,  0.9729]) 21\n",
      "tensor([ 0.2865,  0.4282, -0.3007,  0.9635]) 22\n",
      "tensor([0.5209, 0.4498, 0.0040, 0.9987]) 23\n",
      "tensor([ 0.4398,  0.4521, -0.0041,  0.9990]) 24\n",
      "tensor([ 0.5327,  0.6763, -0.0036,  0.9822]) 25\n",
      "tensor([0.4480, 0.6772, 0.0047, 0.9835]) 26\n",
      "tensor([0.5395, 0.8703, 0.1541, 0.9681]) 27\n",
      "tensor([0.4535, 0.8682, 0.1882, 0.9690]) 28\n",
      "tensor([0.5270, 0.8957, 0.1568, 0.8887]) 29\n",
      "tensor([0.4640, 0.8917, 0.1934, 0.8342]) 30\n",
      "tensor([ 0.5622,  0.9378, -0.0365,  0.9569]) 31\n",
      "tensor([0.4337, 0.9360, 0.0058, 0.9546]) 32\n"
     ]
    }
   ],
   "source": [
    "avg_tensor = torch.mean(tensor_data[:, :, :], dim=0)\n",
    "for i,tns in enumerate(avg_tensor):\n",
    "    print(tns, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49695b6b-9b41-435c-8cb8-55488bef2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(avg_tensor, 'avg_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2450015-6220-46ae-abc7-a3ec406f1958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair is the two landmarks we want to measure between\n",
    "def get_distance_between_two_t(avg_tensor, pair):\n",
    "    l1, l2 = pair[0], pair[1]\n",
    "    \n",
    "    i1_x = avg_tensor[l1][0]\n",
    "    i1_y = avg_tensor[l1][1]\n",
    "    i2_x = avg_tensor[l2][0]\n",
    "    i2_y = avg_tensor[l2][1]\n",
    "\n",
    "    return math.sqrt((i1_x - i2_x)**2 + (i1_y - i2_y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32776e0b-3263-4165-b864-b55fe6310ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_t(avg_tensor, landmarks):\n",
    "    distance = 0\n",
    "    \n",
    "    for i in range(len(landmarks) - 1):  # Iterate up to the second last element\n",
    "        pair = (landmarks[i], landmarks[i+1])  # Access current element and the next one\n",
    "        distance += get_distance_between_two_t(avg_tensor, pair)\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b93aac2-625e-46e1-8872-0cd16199a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_neck_tensor(avg_tensor):\n",
    "    base = torch.zeros(4)\n",
    "    base[0] = (avg_tensor[mp_pose.PoseLandmark.LEFT_SHOULDER.value][0] + avg_tensor[mp_pose.PoseLandmark.RIGHT_SHOULDER.value][0]) / 2\n",
    "    avg_y_mouth = (avg_tensor[mp_pose.PoseLandmark.MOUTH_LEFT.value][1] + avg_tensor[mp_pose.PoseLandmark.MOUTH_RIGHT.value][1]) / 2\n",
    "    avg_y_shoulder = (avg_tensor[mp_pose.PoseLandmark.LEFT_SHOULDER.value][1] + avg_tensor[mp_pose.PoseLandmark.RIGHT_SHOULDER.value][1]) / 2\n",
    "    base[1] = (avg_y_mouth + avg_y_shoulder) / 2\n",
    "    \n",
    "    # print(base)\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81d25878-bf23-4652-a7fd-c60d95383993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sleeve(avg_tensor):\n",
    "    # print(avg_tensor)\n",
    "    d1 = get_distance_t(avg_tensor, [mp_pose.PoseLandmark.LEFT_SHOULDER.value, mp_pose.PoseLandmark.LEFT_ELBOW.value, mp_pose.PoseLandmark.LEFT_WRIST.value])\n",
    "    d2 = get_distance_t(avg_tensor, [mp_pose.PoseLandmark.RIGHT_SHOULDER.value, mp_pose.PoseLandmark.RIGHT_ELBOW.value, mp_pose.PoseLandmark.RIGHT_WRIST.value])\n",
    "\n",
    "    base_tensor = base_neck_tensor(avg_tensor)\n",
    "    print(base_tensor)\n",
    "\n",
    "    d1 += math.sqrt((base_tensor[0] - avg_tensor[mp_pose.PoseLandmark.LEFT_SHOULDER.value][0])**2 + (base_tensor[1] - avg_tensor[mp_pose.PoseLandmark.LEFT_SHOULDER.value][1])**2)\n",
    "    d2 += math.sqrt((base_tensor[0] - avg_tensor[mp_pose.PoseLandmark.RIGHT_SHOULDER.value][0])**2 + (base_tensor[1] - avg_tensor[mp_pose.PoseLandmark.RIGHT_SHOULDER.value][1])**2)\n",
    "\n",
    "    # print(d1, d2)\n",
    "    \n",
    "    # print(d1, d2)\n",
    "    return (d1 + d2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcb76187-15a9-469f-9ede-da3590cb3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_inseam():\n",
    "    d1 = get_distance_t(avg_tensor, [mp_pose.PoseLandmark.LEFT_HIP.value, mp_pose.PoseLandmark.LEFT_KNEE.value, mp_pose.PoseLandmark.LEFT_ANKLE.value])\n",
    "    d2 = get_distance_t(avg_tensor, [mp_pose.PoseLandmark.RIGHT_HIP.value, mp_pose.PoseLandmark.RIGHT_KNEE.value, mp_pose.PoseLandmark.RIGHT_ANKLE.value])\n",
    "    # print(d1, d2)\n",
    "    return (d1 + d2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22bd390e-c1ae-409e-b485-5ba5264e326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_height():\n",
    "    avg_bottom_x = (avg_tensor[mp_pose.PoseLandmark.LEFT_ANKLE.value][0] + avg_tensor[mp_pose.PoseLandmark.RIGHT_ANKLE.value][0]) / 2\n",
    "    avg_bottom_y = (avg_tensor[mp_pose.PoseLandmark.LEFT_ANKLE.value][1] + avg_tensor[mp_pose.PoseLandmark.RIGHT_ANKLE.value][1]) / 2\n",
    "    nose = mp_pose.PoseLandmark.NOSE.value\n",
    "    \n",
    "    return math.sqrt((avg_tensor[nose][0] - avg_bottom_x)**2 + (avg_tensor[nose][1] - avg_bottom_y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db93f527-84c3-40b0-8447-36853b5e5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 74\n",
    "fact = (height - 6) / find_height()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a64737-8e38-4e5b-9dc0-e422fbc5ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_sleeve(avg_tensor) * fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550a79e-bae4-4033-b7c9-a95ed21fdc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_inseam() * fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5f269-06fe-4190-8bfe-2f4e6cfd5c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_height() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a73132-59de-4238-84cc-67bacd91f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chest():\n",
    "    return get_distance_t(avg_tensor, [mp_pose.PoseLandmark.LEFT_SHOULDER.value, mp_pose.PoseLandmark.RIGHT_SHOULDER.value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7aea02ba-f235-4fac-b000-1401a8860b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_waist():\n",
    "    return get_distance_t(avg_tensor, [mp_pose.PoseLandmark.RIGHT_HIP.value, mp_pose.PoseLandmark.LEFT_HIP.value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b68ba75a-43ae-4ee7-9067-10ca6553b348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.05870832317778"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_chest() * fact * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af2b482d-a17e-4dba-ac1b-862a7ec4604b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.403432427980388"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_waist() * fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d24de50-262d-441b-a721-084866a2aff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
