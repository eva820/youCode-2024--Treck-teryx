{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86e6380b-dbea-4e94-9a91-032e6601dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 17:47:25.101435: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-06 17:47:27.530910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "# import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "945212ac-8f6a-4d81-9c39-90f14e5fea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2a29481-114f-413f-9ec1-dd583218016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "# with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "    \n",
    "#         image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         # image.flags.writable = False\n",
    "\n",
    "#         results = pose.process(image)\n",
    "\n",
    "#         # image.flags.writable = True\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#         mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "#         cv2.imshow('Mediapipe feed', image)\n",
    "        \n",
    "#         if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dac1574-6c0e-43ae-911f-bb0987cc3fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({(15, 21), (16, 20), (18, 20), (3, 7), (14, 16), (23, 25), (28, 30), (11, 23), (27, 31), (6, 8), (15, 17), (24, 26), (16, 22), (4, 5), (5, 6), (29, 31), (12, 24), (23, 24), (0, 1), (9, 10), (1, 2), (0, 4), (11, 13), (30, 32), (28, 32), (15, 19), (16, 18), (25, 27), (26, 28), (12, 14), (17, 19), (2, 3), (11, 12), (27, 29), (13, 15)})\n"
     ]
    }
   ],
   "source": [
    "print(mp_pose.POSE_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4739a450-0604-4863-9309-1f68fe383d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({(15, 21), (16, 20), (18, 20), (3, 7), (14, 16), (23, 25), (28, 30), (11, 23), (27, 31), (6, 8), (15, 17), (24, 26), (16, 22), (4, 5), (5, 6), (29, 31), (12, 24), (23, 24), (0, 1), (9, 10), (1, 2), (0, 4), (11, 13), (30, 32), (28, 32), (15, 19), (16, 18), (25, 27), (26, 28), (12, 14), (17, 19), (2, 3), (11, 12), (27, 29), (13, 15)})\n"
     ]
    }
   ],
   "source": [
    "print(mp.solutions.pose.POSE_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09d3a6bc-7d36-44c8-adc8-075081a97ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_in_frame(landmarks):\n",
    "    left = landmarks[mp_pose.PoseLandmark.LEFT_HEEL.value].y\n",
    "    right = landmarks[mp_pose.PoseLandmark.RIGHT_HEEL.value].y\n",
    "    nose = landmarks[mp_pose.PoseLandmark.NOSE.value].y\n",
    "\n",
    "    # heel out of frame\n",
    "    if left > 0.95 or right > 0.95:\n",
    "        return False\n",
    "    # heel wayy to in frame\n",
    "    if left < 0 or right < 0 or nose < 0:\n",
    "        return False\n",
    "    if nose > 0.15:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3930ebf0-6d14-4892-9f8c-ba265fffb18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712450853.760259   59195 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1712450853.824967   59244 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 545.23.08), renderer: NVIDIA GeForce RTX 2060/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "captured_body = False\n",
    "quit = False\n",
    "measurements = []\n",
    "start = None\n",
    "image = None\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # variables to maintain continous stream of data\n",
    "            if not captured_body:\n",
    "                captured_body = True\n",
    "                start = time.time() \n",
    "\n",
    "            # if our body isn't in frame, restart\n",
    "            if not body_in_frame(landmarks):\n",
    "                captured_body = False\n",
    "                start = None\n",
    "                measurements.clear()\n",
    "                \n",
    "            # add landmarks to our current stuff\n",
    "            if captured_body:\n",
    "                measurements.append(landmarks)\n",
    "                \n",
    "                # quit when we have 3 seconds of data\n",
    "                if time.time() - start > 3:\n",
    "                    quit = True\n",
    "            \n",
    "        except:\n",
    "            captured_body = False\n",
    "            start = None\n",
    "            measurements.clear()\n",
    "\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        cv2.imshow('Mediapipe feed', image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q') or quit:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159e1688-ee71-4d64-8b34-36492a8ba1c7",
   "metadata": {},
   "source": [
    "## Turn measurements into a 3d tensor. \n",
    "The tensor should be of shape(`len(measurements)`, `33` (the number of landmarks), `4` (x, y, z, and visisibility))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d38bd27-48a6-4d2d-9331-fe9120b24cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31, 33, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor_data = torch.zeros(len(measurements), 33, 4)\n",
    "print(tensor_data.shape)\n",
    "\n",
    "for idx, itm in enumerate(measurements):\n",
    "    for jdx, jtm in enumerate(itm):\n",
    "        tensor_data[idx][jdx][0] = jtm.x\n",
    "        tensor_data[idx][jdx][1] = jtm.y\n",
    "        tensor_data[idx][jdx][2] = jtm.z\n",
    "        tensor_data[idx][jdx][3] = jtm.visibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f50debb-be74-4f85-a482-4e40aa76855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lndmrk in mp_pose.PoseLandmark:\n",
    "#     print(lndmrk.name, \":\",  lndmrk.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52221312-c05d-44fe-8a10-9cc382bb9d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d98c39d1-61f4-4555-9020-1a3c5f7dffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left_foot = landmarks[mp_pose.PoseLandmark.LEFT_HEEL.value].y\n",
    "# right_foot = landmarks[mp_pose.PoseLandmark.RIGHT_HEEL.value].y\n",
    "# nose = landmarks[mp_pose.PoseLandmark.NOSE.value].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7add7097-ac9f-40c6-81df-eccc24a64e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nose)\n",
    "# print(right_foot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b560a8d-c926-42df-87ec-897ac9fcea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((left_foot + right_foot)/2 - nose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d743e-fe42-46eb-b3d5-aef3290bee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('BGR Image', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bb099fd-b88d-4877-8a76-e7f3cb58e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_tensor = torch.mean(tensor_data[:, :, :], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2450015-6220-46ae-abc7-a3ec406f1958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair is the two landmarks we want to measure between\n",
    "def get_distance_between_two_t(avg_tensor, pair):\n",
    "    l1, l2 = pair[0], pair[1]\n",
    "    \n",
    "    i1_x = avg_tensor[l1][0]\n",
    "    i1_y = avg_tensor[l1][1]\n",
    "    i2_x = avg_tensor[l2][0]\n",
    "    i2_y = avg_tensor[l2][1]\n",
    "\n",
    "    return math.sqrt((i1_x - i2_x)**2 + (i1_y - i2_y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32776e0b-3263-4165-b864-b55fe6310ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_t(avg_tensor, landmarks):\n",
    "    distance = 0\n",
    "    \n",
    "    for i in range(len(landmarks) - 1):  # Iterate up to the second last element\n",
    "        pair = (landmarks[i], landmarks[i+1])  # Access current element and the next one\n",
    "        distance += get_distance_between_two_t(avg_tensor, pair)\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81d25878-bf23-4652-a7fd-c60d95383993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sleeve():\n",
    "    d1 = get_distance_t(avg_tensor, [mp_pose.PoseLandmark.LEFT_SHOULDER.value, mp_pose.PoseLandmark.LEFT_ELBOW.value, mp_pose.PoseLandmark.LEFT_WRIST.value])\n",
    "    d2 = get_distance_t(avg_tensor, [mp_pose.PoseLandmark.RIGHT_SHOULDER.value, mp_pose.PoseLandmark.RIGHT_ELBOW.value, mp_pose.PoseLandmark.RIGHT_WRIST.value])\n",
    "    \n",
    "    # print(d1, d2)\n",
    "    return (d1 + d2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dcb76187-15a9-469f-9ede-da3590cb3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_inseam():\n",
    "    d1 = get_distance_t(avg_tensor, [mp_pose.PoseLandmark.LEFT_HIP.value, mp_pose.PoseLandmark.LEFT_KNEE.value, mp_pose.PoseLandmark.LEFT_ANKLE.value])\n",
    "    d2 = get_distance_t(avg_tensor, [mp_pose.PoseLandmark.RIGHT_HIP.value, mp_pose.PoseLandmark.RIGHT_KNEE.value, mp_pose.PoseLandmark.RIGHT_ANKLE.value])\n",
    "    # print(d1, d2)\n",
    "    return (d1 + d2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64a64737-8e38-4e5b-9dc0-e422fbc5ce58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2234262435209361"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sleeve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a550a79e-bae4-4033-b7c9-a95ed21fdc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33553620850620935"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_inseam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "22bd390e-c1ae-409e-b485-5ba5264e326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_height():\n",
    "    avg_bottom_x = (avg_tensor[mp_pose.PoseLandmark.LEFT_ANKLE.value][0] + avg_tensor[mp_pose.PoseLandmark.RIGHT_ANKLE.value][0]) / 2\n",
    "    avg_bottom_y = (avg_tensor[mp_pose.PoseLandmark.LEFT_ANKLE.value][1] + avg_tensor[mp_pose.PoseLandmark.RIGHT_ANKLE.value][1]) / 2\n",
    "    nose = mp_pose.PoseLandmark.NOSE.value\n",
    "    \n",
    "    return math.sqrt((avg_tensor[nose][0] - avg_bottom_x)**2 + (avg_tensor[nose][1] - avg_bottom_y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16b5f269-06fe-4190-8bfe-2f4e6cfd5c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7229952867467744"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_height()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c2a73132-59de-4238-84cc-67bacd91f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chest():\n",
    "    return get_distance_t(avg_tensor, [mp_pose.PoseLandmark.LEFT_SHOULDER.value, mp_pose.PoseLandmark.RIGHT_SHOULDER.value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b68ba75a-43ae-4ee7-9067-10ca6553b348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1352371591053319"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_chest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b482d-a17e-4dba-ac1b-862a7ec4604b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
